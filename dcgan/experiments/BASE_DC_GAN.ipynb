{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ayz4xex_A754"
      },
      "outputs": [],
      "source": [
        "pip install wandb\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6qJiXinCrtS"
      },
      "source": [
        "BASELINE\n",
        "\n",
        "This is the Baseline DCGAN Model. It's interesting that it's somehow simultaneously upsetting AND exciting to see the discriminator loss and generator loss in equilibrium. Unfortunately, the generated images are pretty bad. It's kind of like watching the two worst teams in a sport compete against each other. It ends up being a good watch because both teams are so competitive that it's entertaining to view. But if you watch the two best teams in that sport play each other, you realize that the first two teams were, in fact, terrible.\n",
        "Also, this poor code looks like it's developing a serious case of mode collapse! The images generated look far too much alike! Nnnnnnooooooo!!!!!\n",
        "\n",
        "Let's see if I can fix it with the first experiment!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZZFtbueBDCq"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"Baseline: Deep Convolutional\n",
        "Generative Adversarial Model (DCGAN)\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import LeakyReLU, Reshape, Dropout\n",
        "from keras.layers import Activation, Dense, Flatten\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "from data.preprocess import load_data_set\n",
        "from tqdm import tqdm_notebook\n",
        "from keras.layers import Conv2D, Conv2DTranspose\n",
        "import os\n",
        "import wandb\n",
        "\n",
        "# Load the dataset\n",
        "(X_train, _), (X_test, _) = mnist.load_data()\n",
        "X = np.vstack((X_train, X_test))\n",
        "X = X.astype('float32')\n",
        "\n",
        "# Normalize: [-1, 1]\n",
        "X = (X - 127.5) / 127.5\n",
        "\n",
        "# Initialize a new wandb run\n",
        "wandb.init(project=\"DCGAN\", name=\"baseline\")\n",
        "\n",
        "def discriminator():\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(64, (3, 3), strides=(2, 2), padding='same', input_shape=(28, 28, 1)))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Conv2D(128, (3, 3), strides=(2, 2), padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(optimizer=Adam(lr=0.0001, beta_1=0.5), loss='binary_crossentropy')\n",
        "\n",
        "    return model\n",
        "\n",
        "def generator(n):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(7*7*128, input_dim=n))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Reshape((7, 7, 128)))\n",
        "    model.add(Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Conv2DTranspose(1, (4, 4), strides=(2, 2), padding='same'))\n",
        "    model.add(Activation('tanh'))\n",
        "\n",
        "    return model\n",
        "\n",
        "def gan(dis, gen):\n",
        "    dis.trainable = False\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(gen)\n",
        "    model.add(dis)\n",
        "\n",
        "    model.compile(optimizer=Adam(lr=0.0002, beta_1=0.5), loss='binary_crossentropy')\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create the logs/baseline directory if it doesn't exist\n",
        "if not os.path.exists('logs/baseline'):\n",
        "    os.makedirs('logs/baseline')\n",
        "\n",
        "discrim = discriminator()\n",
        "geney = generator(100)\n",
        "gan_model = gan(discrim, geney)\n",
        "\n",
        "epochs = 50\n",
        "batch_size = 128\n",
        "half_batch = batch_size // 2\n",
        "n = 100\n",
        "\n",
        "for i in range(epochs):\n",
        "    print(\"EPOCH\", i)\n",
        "    for j in tqdm_notebook(range(len(X) // batch_size)):\n",
        "        # Generate random noise\n",
        "        noise = np.random.normal(0, 1, [half_batch, n])\n",
        "\n",
        "        # Generate fake images\n",
        "        x_fake = geney.predict(noise)\n",
        "\n",
        "        # Use soft labels for training the discriminator\n",
        "        y_real_soft = np.random.uniform(0.9, 1.0, size=(half_batch,))\n",
        "        y_fake_soft = np.random.uniform(0.0, 0.1, size=(half_batch))\n",
        "\n",
        "        # Train discriminator on real and fake data separately\n",
        "        x_real = X[np.random.randint(0, len(X), half_batch)].reshape(half_batch, 28, 28, 1)\n",
        "        d_loss_real = discrim.train_on_batch(x_real + np.random.normal(loc=0.0,scale=0.05,size=x_real.shape), y_real_soft)\n",
        "        d_loss_fake = discrim.train_on_batch(x_fake + np.random.normal(loc=0.0,scale=0.05,size=x_fake.shape), y_fake_soft)\n",
        "\n",
        "        # Calculate the total discriminator loss\n",
        "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "        # Generate new noise for the generator\n",
        "        noise = np.random.normal(0, 1, [batch_size, n])\n",
        "        valid_y = np.array([1] * batch_size)\n",
        "\n",
        "        # Train the generator within the GAN model\n",
        "        g_loss = gan_model.train_on_batch(noise + np.random.normal(loc=0.0,scale=0.05,size=noise.shape), valid_y)\n",
        "\n",
        "    print(\"Discriminator Loss:\", d_loss)\n",
        "    print(\"Generator Loss:\", g_loss)\n",
        "\n",
        "    # Log the losses to wandb\n",
        "    wandb.log({\"Discriminator Loss\": d_loss, \"Generator Loss\": g_loss})\n",
        "\n",
        "    fig, axes = plt.subplots(5, 5)\n",
        "    images = []\n",
        "    for ii in range(5):\n",
        "        for jj in range(5):\n",
        "            img = geney.predict(np.random.randn(1, n)).reshape(28, 28)\n",
        "            axes[ii, jj].imshow(img, cmap='gray')\n",
        "            images.append(wandb.Image(img))\n",
        "    save_path = os.path.join('logs', 'baseline', f'image_at_epoch_{i:04d}.png')\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "    wandb.log({\"Baseline Generated Images\": images})\n",
        "\n",
        "wandb.finish()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
