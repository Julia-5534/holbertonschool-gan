{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHAooXjp-9_p"
      },
      "outputs": [],
      "source": [
        "pip install wandb\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VILFMNclDHbw"
      },
      "source": [
        "EXP2\n",
        "\n",
        "I'm going to start by changing the learning rate of the GAN model. By changing it from 0.0002 to 0.0001, I am halving the step size at each iteration during the training of my model. The learning rate is a hyperparameter that determines how much the weights in the network will be adjusted at each step of the learning process.\n",
        "\n",
        "A smaller learning rate means the model learns slower, which can sometimes lead to better performance because it allows the model to learn more subtle patterns in the data. However, it also means that the model will take longer to train.\n",
        "\n",
        "On the other hand, a larger learning rate means the model learns faster, which can be beneficial if you have a lot of data and are worried about overfitting. But it can also cause the model to miss subtle patterns in the data and can lead to unstable training and poor performance.\n",
        "\n",
        "So, by reducing the learning rate, I'm hopefully making the training process more careful and potentially more accurate, but also slower."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "natMxPmvAEAP"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"Experiment 2: Deep Convolutional\n",
        "Generative Adversarial Model (DCGAN)\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Input, LeakyReLU, Reshape, ReLU\n",
        "from keras.layers import Activation, Dense, Flatten, BatchNormalization\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "from data.preprocess import load_data_set\n",
        "from tqdm import tqdm_notebook\n",
        "from keras.layers import Conv2D, Conv2DTranspose\n",
        "import os\n",
        "import wandb\n",
        "\n",
        "# Load the dataset\n",
        "(X_train, _), (X_test, _) = mnist.load_data()\n",
        "X = np.vstack((X_train, X_test))\n",
        "X = X.astype('float32')\n",
        "\n",
        "# Normalize: [-1, 1]\n",
        "X = (X - 127.5) / 127.5\n",
        "\n",
        "# Initialize a new wandb run\n",
        "wandb.init(project=\"DCGAN\", name=\"exp2\")\n",
        "\n",
        "def discriminator():\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=(28, 28, 1)))\n",
        "    model.add(Conv2D(64, (3, 3), strides=(2, 2), padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    # model.add(Dropout(0.4))\n",
        "    model.add(Conv2D(128, (3, 3), strides=(2, 2), padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    # model.add(Dropout(0.4))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(optimizer=Adam(lr=0.0001, beta_1=0.5), loss='binary_crossentropy')\n",
        "\n",
        "    return model\n",
        "\n",
        "def generator(n):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(7*7*128, input_dim=n))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(ReLU())\n",
        "    model.add(Reshape((7, 7, 128)))\n",
        "    model.add(Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(ReLU())\n",
        "    model.add(Conv2DTranspose(1, (4, 4), strides=(2, 2), padding='same'))\n",
        "    model.add(Activation('tanh'))\n",
        "\n",
        "    return model\n",
        "\n",
        "def gan(dis, gen):\n",
        "    dis.trainable = False\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(gen)\n",
        "    model.add(dis)\n",
        "\n",
        "    model.compile(optimizer=Adam(lr=0.0001, beta_1=0.5), loss='binary_crossentropy')\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create the logs/baseline directory if it doesn't exist\n",
        "if not os.path.exists('logs/exp2'):\n",
        "    os.makedirs('logs/exp2')\n",
        "\n",
        "discrim = discriminator()\n",
        "geney = generator(100)\n",
        "gan_model = gan(discrim, geney)\n",
        "\n",
        "epochs = 50\n",
        "batch_size = 128\n",
        "half_batch = batch_size // 2\n",
        "n = 100\n",
        "\n",
        "for i in range(epochs):\n",
        "    print(\"EPOCH\", i)\n",
        "    for j in tqdm_notebook(range(len(X) // batch_size)):\n",
        "        # Generate random noise\n",
        "        noise = np.random.normal(0, 1, [half_batch, n])\n",
        "\n",
        "        # Generate fake images\n",
        "        x_fake = geney.predict(noise)\n",
        "\n",
        "        # Use soft labels for training the discriminator\n",
        "        y_real_soft = np.random.uniform(0.9, 1.0, size=(half_batch,))\n",
        "        y_fake_soft = np.random.uniform(0.0, 0.1, size=(half_batch))\n",
        "\n",
        "        # Train discriminator on real and fake data separately\n",
        "        x_real = X[np.random.randint(0, len(X), half_batch)].reshape(half_batch, 28, 28, 1)\n",
        "        d_loss_real = discrim.train_on_batch(x_real + np.random.normal(loc=0.0,scale=0.05,size=x_real.shape), y_real_soft)\n",
        "        d_loss_fake = discrim.train_on_batch(x_fake + np.random.normal(loc=0.0,scale=0.05,size=x_fake.shape), y_fake_soft)\n",
        "\n",
        "        # Calculate the total discriminator loss\n",
        "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "        # Generate new noise for the generator\n",
        "        noise = np.random.normal(0, 1, [batch_size, n])\n",
        "        valid_y = np.array([1] * batch_size)\n",
        "\n",
        "        # Train the generator within the GAN model\n",
        "        g_loss = gan_model.train_on_batch(noise + np.random.normal(loc=0.0,scale=0.05,size=noise.shape), valid_y)\n",
        "\n",
        "    print(\"Discriminator Loss:\", d_loss)\n",
        "    print(\"Generator Loss:\", g_loss)\n",
        "\n",
        "    # Log the losses to wandb\n",
        "    wandb.log({\"Discriminator Loss\": d_loss, \"Generator Loss\": g_loss})\n",
        "\n",
        "    fig, axes = plt.subplots(5, 5)\n",
        "    images = []\n",
        "    for ii in range(5):\n",
        "        for jj in range(5):\n",
        "            img = geney.predict(np.random.randn(1, n)).reshape(28, 28)\n",
        "            axes[ii, jj].imshow(img, cmap='gray')\n",
        "            images.append(wandb.Image(img))\n",
        "    save_path = os.path.join('logs', 'exp2', f'image_at_epoch_{i:04d}.png')\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "    wandb.log({\"EXP2 Generated Images\": images})\n",
        "\n",
        "wandb.finish()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3lTMmVZDQkX"
      },
      "source": [
        "RESULTS"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
